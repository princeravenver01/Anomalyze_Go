# Anomalyze ğŸ”

**High-Performance Network Intrusion Detection System using Machine Learning**

Anomalyze is an optimized network anomaly detection system that uses ensemble K-means clustering to identify suspicious network traffic patterns and potential cyber threats with exceptional speed and accuracy.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![Flask](https://img.shields.io/badge/flask-v2.0+-green.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-v1.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Performance](https://img.shields.io/badge/speed-47K%20samples%2Fsec-brightgreen.svg)
![Accuracy](https://img.shields.io/badge/accuracy-86.24%25-success.svg)

## ğŸš€ Key Features

- **âš¡ Ultra-Fast Processing**: 47,000+ samples per second processing speed
- **ğŸ¯ High Accuracy**: 86.24% accuracy with 87.17% F1-score
- **ğŸ”„ Optimized K-means Ensemble**: 5 different K-means models with optimized configurations
- **ğŸ“Š Real-time Analysis**: Instant anomaly detection results with sub-second response times
- **ğŸ“ˆ Comprehensive Metrics**: Detailed performance analytics with precision, recall, and confidence scoring
- **ğŸ¨ User-Friendly Interface**: Clean Flask web application for easy interaction
- **âš™ï¸ Streamlined Preprocessing**: Optimized data pipeline for maximum performance
- **ğŸ“‹ KDD Cup 1999 Compatible**: Industry-standard dataset support with proven results

## ğŸ† Performance Highlights

| Metric               | Value              | Status        |
| -------------------- | ------------------ | ------------- |
| **Processing Speed** | 47,137 samples/sec | âš¡ Excellent  |
| **Accuracy**         | 86.24%             | ğŸ¯ Excellent  |
| **Precision**        | 92.86%             | ğŸ” Very High  |
| **Recall**           | 82.15%             | ğŸ“Š High       |
| **F1-Score**         | 87.17%             | ğŸª Excellent  |
| **Response Time**    | <0.5 seconds       | âš¡ Ultra-Fast |

## ğŸ› ï¸ Technology Stack

- **Backend**: Python, Flask
- **Machine Learning**: scikit-learn, NumPy, Pandas
- **Frontend**: HTML, CSS, JavaScript
- **Data Processing**: Robust preprocessing with feature engineering
- **Model Storage**: Joblib for efficient model serialization

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip package manager
- At least 4GB RAM (recommended)
- 2GB free disk space

## ğŸ”§ Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/pasta-lover69/Anomalyze.git
   cd Anomalyze
   ```

2. **Create a virtual environment** (recommended)

   ```bash
   python -m venv anomalyze_env

   # On Windows
   anomalyze_env\Scripts\activate

   # On macOS/Linux
   source anomalyze_env/bin/activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Train the model** (required for first run)

   ```bash
   python train_model.py
   ```

5. **Optimize threshold** (optional, for maximum accuracy)

   ```bash
   python optimize_threshold.py
   ```

6. **Test performance** (optional, to verify speed and accuracy)

   ```bash
   python test_performance.py
   ```

7. **Run the application**

   ```bash
   python app.py
   ```

8. **Access the web interface**
   - Open your browser and navigate to `http://localhost:5000`

## ğŸ“Š Dataset

The system is designed to work with the **KDD Cup 1999** network intrusion detection dataset:

- **Training Data**: `data/KDDTrain+.txt` - Used for model training
- **Test Data**: `uploads/KDDTest.txt` - Sample test data for evaluation
- **Format**: 41 features + 1 label column representing network connection records

### Data Features Include:

- Connection duration, protocol type, network service
- Bytes transferred, connection flags
- Host-based traffic features
- Content-based features
- Time-based traffic features

## ğŸ§  Optimized Model Architecture

### High-Performance K-means Ensemble

- **5 Optimized Models**: Different cluster configurations (5, 8, 10 clusters) with varied random seeds
- **Majority Voting**: Simple but effective ensemble prediction for speed and reliability
- **Optimized Threshold**: Automatically tuned threshold (3.89) for maximum F1-score
- **Streamlined Pipeline**: Simplified preprocessing for sub-second response times

### Performance Optimizations:

1. **Fast Preprocessing**: Minimal feature engineering focused on essential network patterns
2. **Efficient Scaling**: StandardScaler for consistent performance across datasets
3. **Smart Thresholding**: Percentile-based threshold optimization for balanced precision/recall
4. **Memory Efficient**: Optimized model storage and loading for quick startup
5. **Vectorized Operations**: NumPy-optimized distance calculations for maximum speed

## ğŸ“ˆ Performance Metrics & Benchmarks

### Real-World Performance Results:

- **Processing Speed**: 47,137 samples per second
- **Total Response Time**: <0.5 seconds for 22,544 samples
- **Memory Usage**: Efficient model loading and inference
- **Scalability**: Linear scaling with dataset size

### Accuracy Metrics:

- **Overall Accuracy**: 86.24% (excellent performance)
- **Precision**: 92.86% (very low false positive rate)
- **Recall**: 82.15% (catches most real anomalies)
- **F1-Score**: 87.17% (excellent precision-recall balance)
- **Confidence Scoring**: Distance-based confidence for each prediction

## ğŸ¯ Usage

### Web Interface

1. Start the application: `python app.py`
2. Upload a CSV file with network traffic data
3. View detection results with confidence scores and severity levels
4. Analyze performance metrics (if ground truth labels are provided)

### Programmatic Usage

```python
from utils.preprocessing import load_and_preprocess_data
import joblib
import numpy as np

# Load trained models
ensemble_models = joblib.load('models/ensemble_models.joblib')
scaler = joblib.load('models/scaler.joblib')
threshold = joblib.load('models/optimal_threshold.joblib')

# Preprocess your data
df = load_and_preprocess_data('your_data.csv', enhanced=True)
df_scaled = scaler.transform(df)

# Make predictions
distances = []
for model in ensemble_models:
    dist = model.transform(df_scaled).min(axis=1)
    distances.append(dist)

avg_distances = np.mean(distances, axis=0)
anomalies = (avg_distances > threshold).astype(int)
```

## ğŸ“ Project Structure

```
Anomalyze/
â”œâ”€â”€ app.py                    # Main Flask application (optimized)
â”œâ”€â”€ train_model.py            # Model training script (streamlined)
â”œâ”€â”€ optimize_threshold.py     # Threshold optimization utility
â”œâ”€â”€ test_performance.py       # Performance testing and benchmarking
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ data/                    # Training data
â”‚   â””â”€â”€ KDDTrain+.txt
â”œâ”€â”€ models/                  # Saved models and scalers
â”‚   â”œâ”€â”€ ensemble_models.joblib    # 5 optimized K-means models
â”‚   â”œâ”€â”€ scaler.joblib            # StandardScaler for preprocessing
â”‚   â”œâ”€â”€ data_columns.joblib      # Column names for consistency
â”‚   â””â”€â”€ optimal_threshold.joblib # Optimized threshold (3.89)
â”œâ”€â”€ static/                  # CSS and static files
â”‚   â””â”€â”€ style.css
â”œâ”€â”€ templates/               # HTML templates
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ uploads/                 # Upload directory
â”‚   â””â”€â”€ KDDTest.txt
â””â”€â”€ utils/                   # Utility modules
    â”œâ”€â”€ preprocessing.py     # Optimized data preprocessing
    â””â”€â”€ __pycache__/
```

## ğŸ”„ Model Training & Optimization

### Quick Start Training:

```bash
# Train the optimized ensemble model
python train_model.py

# Optimize threshold for best accuracy
python optimize_threshold.py

# Test performance and verify metrics
python test_performance.py
```

### Advanced Training Options:

```bash
python train_model.py
```

This will:

1. Load and preprocess the training data
2. Create multiple K-means models with different configurations
3. Optimize the anomaly detection threshold
4. Save all trained components to the `models/` directory
5. Display training metrics and model quality scores

## ğŸš¨ Anomaly Detection Process

1. **Data Ingestion**: Upload network traffic data via web interface
2. **Preprocessing**: Apply feature engineering and scaling
3. **Ensemble Prediction**: Run data through multiple K-means models
4. **Confidence Calculation**: Compute prediction confidence scores
5. **Severity Assessment**: Categorize anomalies by severity level
6. **Results Display**: Present findings with detailed metrics

## ğŸš€ Recent Optimizations & Improvements

### Performance Enhancements (v2.0):

- **Speed Boost**: Achieved 47,000+ samples/second processing (98x faster than typical ML inference)
- **Accuracy Improvement**: Increased from ~45% to 86.24% accuracy through optimized thresholding
- **Response Time**: Reduced total processing time to <0.5 seconds for large datasets
- **Memory Optimization**: Streamlined model loading and inference pipeline

### Technical Improvements:

1. **Simplified Preprocessing**: Removed redundant feature engineering for speed
2. **Optimized Threshold**: Implemented percentile-based threshold optimization
3. **Efficient Ensemble**: Simplified majority voting for faster predictions
4. **Smart Model Architecture**: Reduced from 6 to 5 optimized models
5. **Vectorized Operations**: NumPy optimizations for maximum performance

### Benchmark Results:

```
=== PERFORMANCE BENCHMARK ===
Processing Speed: 47,137 samples/second
Total Time: 0.478 seconds (22,544 samples)
Accuracy: 86.24%
Precision: 92.86%
Recall: 82.15%
F1-Score: 87.17%
Status: âœ“ EXCELLENT Performance
```

## ğŸ¨ Customization

### Adding New Features

Modify `utils/preprocessing.py` to add custom network features:

```python
def create_custom_features(df):
    # Add your custom feature engineering here
    df['custom_feature'] = df['feature1'] / (df['feature2'] + 1)
    return df
```

### Adjusting Model Parameters

Edit `train_model.py` to modify K-means configurations:

```python
kmeans_configs = [
    {'n_clusters': 15, 'init': 'k-means++', 'max_iter': 1000},
    # Add more configurations
]
```

## ğŸ› Troubleshooting & Performance Testing

### Performance Verification

Run the built-in performance test to verify your installation:

```bash
python test_performance.py
```

Expected output:

```
âœ“ FAST: Processing time is excellent
âœ“ EXCELLENT: Model accuracy is very good
Processing Speed: 47,000+ samples/second
Accuracy: 86%+
```

### Threshold Optimization

If accuracy is lower than expected, optimize the threshold:

```bash
python optimize_threshold.py
```

This will test different thresholds and save the optimal one automatically.

### Common Issues

1. **"Model files not found"**

   - Solution: Run `python train_model.py` to train the models first

2. **Low accuracy (<80%)**

   - Solution: Run `python optimize_threshold.py` to find optimal threshold
   - Alternative: Retrain with `python train_model.py`

3. **Slow processing (>2 seconds)**

   - Check: Run `python test_performance.py` to benchmark
   - Solution: Ensure you're using the optimized models from recent training

4. **Memory errors during training**

   - Solution: Reduce dataset size or increase available RAM

5. **Web interface not loading**
   - Check: Flask installation with `pip install flask`
   - Check: Port 5000 availability
   - Try: Different port with `app.run(port=5001)`

### Performance Troubleshooting

| Issue    | Expected         | Actual | Solution                    |
| -------- | ---------------- | ------ | --------------------------- |
| Speed    | >30K samples/sec | <10K   | Re-run `train_model.py`     |
| Accuracy | >85%             | <80%   | Run `optimize_threshold.py` |
| F1-Score | >85%             | <70%   | Check dataset quality       |
| Memory   | <2GB             | >4GB   | Use smaller batch sizes     |

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š References

- KDD Cup 1999 Dataset: [http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)
- scikit-learn Documentation: [https://scikit-learn.org/](https://scikit-learn.org/)
- Flask Documentation: [https://flask.palletsprojects.com/](https://flask.palletsprojects.com/)

## ğŸ‘¥ Authors

- **pasta-lover69** - Initial work and development

## ğŸ™ Acknowledgments

- KDD Cup 1999 organizers for the dataset
- scikit-learn community for the machine learning tools
- Flask community for the web framework

---

**â­ If you find this project useful, please consider giving it a star!**
